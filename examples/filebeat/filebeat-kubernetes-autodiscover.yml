filebeat.autodiscover:
  providers:
  - type: kubernetes
    node: ${NODE_NAME:}
    hints.enabled: true
    hints.default_config:
      type: filestream
      paths:
      - /var/log/containers/*${data.kubernetes.container.id}.log

processors:
- add_kubernetes_metadata: ~
- add_fields:
    target: data_stream
    fields:
      type: ${DATA_STREAM_TYPE:logs}
      dataset: ${DATA_STREAM_DATASET:kubernetes.generic}
      namespace: ${DATA_STREAM_NAMESPACE:default}
- add_fields:
    target: service
    fields:
      name: ${SERVICE_NAME:filebeat}
- add_host_metadata: ~
- add_cloud_metadata: ~
- add_docker_metadata: ~

setup.template.enabled: false
setup.ilm.enabled: false

# Default: Elasticsearch output to data stream index name
output.elasticsearch:
  hosts: [ "${ES_HOSTS:https://localhost:9200}" ]
  username: "${ES_USERNAME:elastic}"
  password: "${ES_PASSWORD:changeme}"
  ssl.verification_mode: ${ES_SSL_VERIFY:full}
  index: "logs-%{[data_stream.dataset]}-%{[data_stream.namespace]}"

# Alternative: Logstash output (uncomment to use, and comment out the Elasticsearch output above)
#output.logstash:
#  hosts: [ "${LOGSTASH_HOSTS:localhost:5044}" ]
#  #loadbalance: true
#  #ssl.enabled: false
#  #ssl.certificate_authorities: ["/etc/pki/tls/certs/ca-bundle.crt"]

# Alternative: Kafka output (uncomment to use, and comment out the Elasticsearch output above)
#output.kafka:
#  hosts: [ "${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}" ]
#  topic: "${KAFKA_TOPIC:logs}"
#  partition.round_robin.reachable_only: true
#  required_acks: 1
#  compression: gzip
#  max_message_bytes: 1000000
#  codec.json:
#    pretty: false

logging.level: info
