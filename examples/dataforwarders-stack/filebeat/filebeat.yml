filebeat.inputs:
- type: filestream
  id: local-logs
  enabled: true
  paths:
  - /var/log/*.log
  - /var/log/syslog
  - /var/log/messages
  ignore_older: 72h
  exclude_files: [ '\.gz$' ]

processors:
- decode_json_fields:
    fields: [ "message" ]
    process_array: true
    max_depth: 5
    target: ""
    overwrite_keys: true
    add_error_key: true
    when:
      regexp:
        message: '^\{'
- add_fields:
    target: data_stream
    fields:
      type: ${DATA_STREAM_TYPE:logs}
      dataset: ${DATA_STREAM_DATASET:filebeat.generic}
      namespace: ${DATA_STREAM_NAMESPACE:default}
- add_fields:
    target: service
    fields:
      name: ${SERVICE_NAME:filebeat}
- add_host_metadata: ~
- add_cloud_metadata: ~
- add_docker_metadata: ~

setup.template.enabled: false
setup.ilm.enabled: false

# Default: Elasticsearch output to data stream index name
output.elasticsearch:
  hosts: [ "${ES_HOSTS:https://localhost:9200}" ]
  username: "${ES_USERNAME:elastic}"
  password: "${ES_PASSWORD:changeme}"
  ssl.verification_mode: ${ES_SSL_VERIFY:full}
  index: "logs-%{[data_stream.dataset]}-%{[data_stream.namespace]}"

# Alternative: Logstash output (uncomment to use)
#output.logstash:
#  hosts: [ "${LOGSTASH_HOSTS:localhost:5044}" ]
#  #loadbalance: true
#  #ssl.enabled: false
#  #ssl.certificate_authorities: ["/etc/pki/tls/certs/ca-bundle.crt"]

# Alternative: Kafka output (uncomment to use)
#output.kafka:
#  hosts: [ "${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}" ]
#  topic: "${KAFKA_TOPIC:logs}"
#  partition.round_robin.reachable_only: true
#  required_acks: 1
#  compression: gzip
#  max_message_bytes: 1000000
#  codec.json:
#    pretty: false

logging.level: info
logging.json: true
logging.files:
  path: /var/log/filebeat
  name: filebeat
  keepfiles: 7
  rotateeverybytes: 10485760

http.enabled: true
http.host: localhost
http.port: 5066
