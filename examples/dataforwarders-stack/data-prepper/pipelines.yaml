# Example Data Prepper pipelines mirroring Logstash examples
# Run with Docker (see README) or local Data Prepper install

http_ingest:
  workers: ${HTTP_WORKERS:2}
  delay: ${HTTP_DELAY_MS:100}
  source:
    http:
      port: ${HTTP_PORT:2021}
      request_timeout: ${HTTP_REQUEST_TIMEOUT_MS:30000}
      max_request_size: ${HTTP_MAX_REQUEST_BYTES:1048576}
  sink:
  - opensearch:
      hosts: [ "${OPENSEARCH_HOSTS:https://localhost:9200}" ]
      username: "${OPENSEARCH_USERNAME:admin}"
      password: "${OPENSEARCH_PASSWORD:admin}"
      index: "${OS_INDEX_HTTP:logs-http-ingest}"
      ssl: ${OS_SSL:true}
      insecure: ${OS_INSECURE:false}
      bulk_size: ${OS_BULK_SIZE_MB:5}
      max_retries: ${OS_MAX_RETRIES:3}

kafka:
  workers: ${KAFKA_WORKERS:2}
  delay: ${KAFKA_DELAY_MS:100}
  source:
    kafka:
      bootstrap_servers: "${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}"
      topics: [ "${KAFKA_TOPICS:logs}" ]
      group_id: "${KAFKA_GROUP_ID:data-prepper}"
      auto_offset_reset: "${KAFKA_AUTO_OFFSET_RESET:latest}"
      # If your payloads are JSON, set this to true to parse bodies
      deserialize_json: ${KAFKA_JSON:true}
  sink:
  - opensearch:
      hosts: [ "${OPENSEARCH_HOSTS:https://localhost:9200}" ]
      username: "${OPENSEARCH_USERNAME:admin}"
      password: "${OPENSEARCH_PASSWORD:admin}"
      index: "${OS_INDEX_KAFKA:logs-kafka-generic}"
      ssl: ${OS_SSL:true}
      insecure: ${OS_INSECURE:false}

stdin_like:
  workers: ${STDIN_WORKERS:1}
  delay: ${STDIN_DELAY_MS:100}
  # Data Prepper does not have a stdin source; emulate via dedicated HTTP port
  source:
    http:
      port: ${STDIN_HTTP_PORT:2023}
  sink:
  - opensearch:
      hosts: [ "${OPENSEARCH_HOSTS:https://localhost:9200}" ]
      username: "${OPENSEARCH_USERNAME:admin}"
      password: "${OPENSEARCH_PASSWORD:admin}"
      index: "${OS_INDEX_STDIN:logs-stdin-generic}"
      ssl: ${OS_SSL:true}
      insecure: ${OS_INSECURE:false}

syslog_like:
  workers: ${SYSLOG_WORKERS:2}
  delay: ${SYSLOG_DELAY_MS:100}
  # Data Prepper may not provide a native syslog source. Use HTTP.
  source:
    http:
      port: ${SYSLOG_HTTP_PORT:2024}
  sink:
  - opensearch:
      hosts: [ "${OPENSEARCH_HOSTS:https://localhost:9200}" ]
      username: "${OPENSEARCH_USERNAME:admin}"
      password: "${OPENSEARCH_PASSWORD:admin}"
      index: "${OS_INDEX_SYSLOG:logs-syslog-rfc3164}"
      ssl: ${OS_SSL:true}
      insecure: ${OS_INSECURE:false}

beats_like:
  workers: ${BEATS_WORKERS:2}
  delay: ${BEATS_DELAY_MS:100}
  # Emulate Beats via HTTP JSON intake on a separate port
  source:
    http:
      port: ${BEATS_HTTP_PORT:2022}
  sink:
  - opensearch:
      hosts: [ "${OPENSEARCH_HOSTS:https://localhost:9200}" ]
      username: "${OPENSEARCH_USERNAME:admin}"
      password: "${OPENSEARCH_PASSWORD:admin}"
      index: "${OS_INDEX_BEATS:logs-filebeat-generic}"
      ssl: ${OS_SSL:true}
      insecure: ${OS_INSECURE:false}

# Advanced: Kafka â†’ OpenSearch and Elasticsearch (dual sinks)
kafka_dual:
  workers: ${KAFKA_WORKERS:2}
  delay: ${KAFKA_DELAY_MS:100}
  source:
    kafka:
      bootstrap_servers: "${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}"
      topics: [ "${KAFKA_TOPICS:logs}" ]
      group_id: "${KAFKA_GROUP_ID:data-prepper}"
      auto_offset_reset: "${KAFKA_AUTO_OFFSET_RESET:latest}"
      deserialize_json: ${KAFKA_JSON:true}
  sink:
  - opensearch:
      hosts: [ "${OPENSEARCH_HOSTS:https://localhost:9200}" ]
      username: "${OPENSEARCH_USERNAME:admin}"
      password: "${OPENSEARCH_PASSWORD:admin}"
      index: "${OS_INDEX_KAFKA_DUAL_OS:logs-kafka-os}"
      ssl: ${OS_SSL:true}
      insecure: ${OS_INSECURE:false}
      bulk_size: ${OS_BULK_SIZE_MB:5}
      max_retries: ${OS_MAX_RETRIES:3}
  - opensearch:
      hosts: [ "${ELASTICSEARCH_HOSTS:https://localhost:9201}" ]
      username: "${ELASTICSEARCH_USERNAME:elastic}"
      password: "${ELASTICSEARCH_PASSWORD:changeme}"
      index: "${ES_INDEX_KAFKA_DUAL_ES:logs-kafka-es}"
      ssl: ${ES_SSL:true}
      insecure: ${ES_INSECURE:false}
      bulk_size: ${ES_BULK_SIZE_MB:5}
      max_retries: ${ES_MAX_RETRIES:3}

# Filter examples (Data Prepper processors) mirroring Logstash filter examples
filters_syslog_auth:
  workers: ${FILTER_WORKERS:2}
  delay: ${FILTER_DELAY_MS:100}
  source:
    http:
      port: ${FILTER_SYSLOG_HTTP_PORT:2124}
  processor:
  - grok:
      match:
        message: [ '%{SYSLOGTIMESTAMP:syslog_timestamp} %{HOSTNAME:host} sshd\[%{NUMBER:pid}\]: %{GREEDYDATA:sshd_message}', '%{SYSLOGTIMESTAMP:syslog_timestamp} %{HOSTNAME:host} sudo\[%{NUMBER:pid}\]: %{GREEDYDATA:sudo_message}' ]
  - grok:
      match:
        sshd_message: [ "Accepted %{WORD:ssh_method} for %{USERNAME:user.name} from %{IP:source.ip} port %{NUMBER:source.port:int}" ]
      tag_on_failure: [ "_grok_sshd_parse_failure" ]
  - date:
      from_time_received: false
      match: [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
  sink:
  - opensearch:
      hosts: [ "${OPENSEARCH_HOSTS:https://localhost:9200}" ]
      username: "${OPENSEARCH_USERNAME:admin}"
      password: "${OPENSEARCH_PASSWORD:admin}"
      index: "${OS_INDEX_FILTER_SYSLOG:logs-syslog-auth}"
      ssl: ${OS_SSL:true}
      insecure: ${OS_INSECURE:false}

filters_apache_access:
  workers: ${FILTER_WORKERS:2}
  delay: ${FILTER_DELAY_MS:100}
  source:
    http:
      port: ${FILTER_APACHE_HTTP_PORT:2121}
  processor:
  - grok:
      match:
        message: [ "%{COMBINEDAPACHELOG}" ]
      tag_on_failure: [ "_grok_apache_failure" ]
  - rename_keys:
      mappings:
        clientip: source.ip
  - user_agent:
      source: agent
      target: user_agent
  - geoip:
      source: source.ip
      target: geoip
  sink:
  - opensearch:
      hosts: [ "${OPENSEARCH_HOSTS:https://localhost:9200}" ]
      username: "${OPENSEARCH_USERNAME:admin}"
      password: "${OPENSEARCH_PASSWORD:admin}"
      index: "${OS_INDEX_FILTER_APACHE:logs-apache-access}"
      ssl: ${OS_SSL:true}
      insecure: ${OS_INSECURE:false}

filters_json_app:
  workers: ${FILTER_WORKERS:2}
  delay: ${FILTER_DELAY_MS:100}
  source:
    http:
      port: ${FILTER_JSON_HTTP_PORT:2122}
  processor:
  - json:
      source: message
  - date:
      match: [ "@timestamp", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSSZZ" ]
      target: "@timestamp"
      timezone: "UTC"
  - lowercase_strings:
      keys: [ "level" ]
  - rename_keys:
      mappings:
        host: host.name
  sink:
  - opensearch:
      hosts: [ "${OPENSEARCH_HOSTS:https://localhost:9200}" ]
      username: "${OPENSEARCH_USERNAME:admin}"
      password: "${OPENSEARCH_PASSWORD:admin}"
      index: "${OS_INDEX_FILTER_JSON:logs-app-json}"
      ssl: ${OS_SSL:true}
      insecure: ${OS_INSECURE:false}

filters_k8s_container:
  workers: ${FILTER_WORKERS:2}
  delay: ${FILTER_DELAY_MS:100}
  source:
    http:
      port: ${FILTER_K8S_HTTP_PORT:2123}
  processor:
  - dissect:
      patterns:
        message: "%{ts->} %{stream} %{flags} %{log}"
      add_tags: [ "_cri_dissect" ]
  - date:
      match: [ "ts", "ISO8601" ]
      target: "@timestamp"
  - json:
      source: log
      target: ""
      ignore_invalid_json: true
  - remove_keys:
      keys: [ "flags", "host", "@version" ]
  sink:
  - opensearch:
      hosts: [ "${OPENSEARCH_HOSTS:https://localhost:9200}" ]
      username: "${OPENSEARCH_USERNAME:admin}"
      password: "${OPENSEARCH_PASSWORD:admin}"
      index: "${OS_INDEX_FILTER_K8S:logs-kubernetes-container}"
      ssl: ${OS_SSL:true}
      insecure: ${OS_INSECURE:false}

filters_nginx_access:
  workers: ${FILTER_WORKERS:2}
  delay: ${FILTER_DELAY_MS:100}
  source:
    http:
      port: ${FILTER_NGINX_HTTP_PORT:2125}
  processor:
  - grok:
      match:
        message: [ '%{IPORHOST:clientip} - %{DATA:ident} %{DATA:auth} \[%{HTTPDATE:nginx.access.time}\] \"%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:response:int} %{NUMBER:bytes:int} \"%{DATA:referrer}\" \"%{DATA:agent}\"' ]
      tag_on_failure: [ "_grok_nginx_failure" ]
  - rename_keys:
      mappings:
        clientip: source.ip
  - user_agent:
      source: agent
      target: user_agent
  - geoip:
      source: source.ip
      target: geoip
  sink:
  - opensearch:
      hosts: [ "${OPENSEARCH_HOSTS:https://localhost:9200}" ]
      username: "${OPENSEARCH_USERNAME:admin}"
      password: "${OPENSEARCH_PASSWORD:admin}"
      index: "${OS_INDEX_FILTER_NGINX:logs-nginx-access}"
      ssl: ${OS_SSL:true}
      insecure: ${OS_INSECURE:false}

filters_nginx_ingress:
  workers: ${FILTER_WORKERS:2}
  delay: ${FILTER_DELAY_MS:100}
  source:
    http:
      port: ${FILTER_INGRESS_HTTP_PORT:2126}
  processor:
  - grok:
      match:
        message: [ '%{IPORHOST:clientip} - %{DATA:remote_user} \[%{HTTPDATE:time_local}\] \"%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:status:int} %{NUMBER:body_bytes_sent:int} \"%{DATA:referrer}\" \"%{DATA:agent}\" %{NUMBER:request_length:int} %{NUMBER:request_time:float} \[%{DATA:upstream_status}\] \[%{DATA:upstream_addr}\] \[%{NUMBER:upstream_response_time:float}\] %{DATA:namespace}/%{DATA:ingress}/%{DATA:service}/%{DATA:pod} %{DATA:host} %{IPORHOST:server_addr}:%{NUMBER:server_port:int}' ]
      tag_on_failure: [ "_grok_ingress_failure" ]
  - rename_keys:
      mappings:
        clientip: source.ip
  - user_agent:
      source: agent
      target: user_agent
  - geoip:
      source: source.ip
      target: geoip
  sink:
  - opensearch:
      hosts: [ "${OPENSEARCH_HOSTS:https://localhost:9200}" ]
      username: "${OPENSEARCH_USERNAME:admin}"
      password: "${OPENSEARCH_PASSWORD:admin}"
      index: "${OS_INDEX_FILTER_INGRESS:logs-nginx-ingress}"
      ssl: ${OS_SSL:true}
      insecure: ${OS_INSECURE:false}

filters_log_type_detector:
  workers: ${FILTER_WORKERS:2}
  delay: ${FILTER_DELAY_MS:100}
  source:
    http:
      port: ${FILTER_DETECT_HTTP_PORT:2130}
  # Detect common log formats and apply per-type processors, add tags
  processor:
  # Apache access
  - grok:
      match:
        message: [ "%{COMBINEDAPACHELOG}" ]
      tag_on_failure: [ "_not_apache" ]
  - add_tags:
      tags: [ "log_type:apache_access" ]
  - rename_keys:
      mappings:
        clientip: source.ip
  - user_agent:
      source: agent
      target: user_agent
  - geoip:
      source: source.ip
      target: geoip
  # Nginx access
  - grok:
      match:
        message: [ "%{IPORHOST:clientip} - %{DATA:ident} %{DATA:auth} \\[ %{HTTPDATE:nginx.access.time} \\] \"%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:response:int} %{NUMBER:bytes:int} \"%{DATA:referrer}\" \"%{DATA:agent}\"" ]
      tag_on_failure: [ "_not_nginx_access" ]
  - add_tags:
      tags: [ "log_type:nginx_access" ]
  - rename_keys:
      mappings:
        clientip: source.ip
  - user_agent:
      source: agent
      target: user_agent
  - geoip:
      source: source.ip
      target: geoip
  # Nginx ingress
  - grok:
      match:
        message: [ "%{IPORHOST:clientip} - %{DATA:remote_user} \\[ %{HTTPDATE:time_local} \\] \"%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:status:int} %{NUMBER:body_bytes_sent:int} \"%{DATA:referrer}\" \"%{DATA:agent}\" %{NUMBER:request_length:int} %{NUMBER:request_time:float} \\[ %{DATA:upstream_status} \\] \\[ %{DATA:upstream_addr} \\] \\[ %{NUMBER:upstream_response_time:float} \\] %{DATA:namespace}/%{DATA:ingress}/%{DATA:service}/%{DATA:pod} %{DATA:host} %{IPORHOST:server_addr}:%{NUMBER:server_port:int}" ]
      tag_on_failure: [ "_not_nginx_ingress" ]
  - add_tags:
      tags: [ "log_type:nginx_ingress" ]
  - rename_keys:
      mappings:
        clientip: source.ip
  - user_agent:
      source: agent
      target: user_agent
  - geoip:
      source: source.ip
      target: geoip
  # Syslog auth
  - grok:
      match:
        message: [ "%{SYSLOGTIMESTAMP:syslog_timestamp} %{HOSTNAME:host} sshd\\[%{NUMBER:pid}\\]: %{GREEDYDATA:sshd_message}", "%{SYSLOGTIMESTAMP:syslog_timestamp} %{HOSTNAME:host} sudo\\[%{NUMBER:pid}\\]: %{GREEDYDATA:sudo_message}" ]
      tag_on_failure: [ "_not_syslog_auth" ]
  - add_tags:
      tags: [ "log_type:syslog_auth" ]
  - grok:
      match:
        sshd_message: [ "Accepted %{WORD:ssh_method} for %{USERNAME:user.name} from %{IP:source.ip} port %{NUMBER:source.port:int}" ]
      tag_on_failure: [ "_grok_sshd_parse_failure" ]
  - date:
      from_time_received: false
      match: [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
  # K8s container (CRI)
  - dissect:
      patterns:
        message: "%{ts->} %{stream} %{flags} %{log}"
      add_tags: [ "_cri_dissect" ]
  - add_tags:
      tags: [ "log_type:k8s_container" ]
  - date:
      match: [ "ts", "ISO8601" ]
      target: "@timestamp"
  - json:
      source: log
      target: ""
      ignore_invalid_json: true
  - remove_keys:
      keys: [ "flags", "host", "@version" ]
  # Generic JSON app
  - json:
      source: message
  - add_tags:
      tags: [ "log_type:json_app" ]
  - date:
      match: [ "@timestamp", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSSZZ" ]
      target: "@timestamp"
      timezone: "UTC"
  - lowercase_strings:
      keys: [ "level" ]
  - rename_keys:
      mappings:
        host: host.name
  sink:
  - opensearch:
      hosts: [ "${OPENSEARCH_HOSTS:https://localhost:9200}" ]
      username: "${OPENSEARCH_USERNAME:admin}"
      password: "${OPENSEARCH_PASSWORD:admin}"
      index: "${OS_INDEX_FILTER_DETECT:logs-log-type-detected}"
      ssl: ${OS_SSL:true}
      insecure: ${OS_INSECURE:false}

filters_log_level_detector:
  workers: ${FILTER_WORKERS:2}
  delay: ${FILTER_DELAY_MS:100}
  source:
    http:
      port: ${FILTER_LEVEL_HTTP_PORT:2131}
  processor:
  - json:
      source: message
      target: ""
      ignore_invalid_json: true
  - grok:
      match:
        message: [ ".*level=%{LOGLEVEL:level}.*", ".*lvl=%{LOGLEVEL:level}.*", ".*severity=%{LOGLEVEL:level}.*" ]
      tag_on_failure: [ "_no_kv_level" ]
  - grok:
      match:
        message: [ '%{TIMESTAMP_ISO8601:ts} %{LOGLEVEL:level} %{GREEDYDATA:log_message}', '%{TIMESTAMP_ISO8601:ts} \[%{LOGLEVEL:level}\] %{GREEDYDATA:log_message}', '\[%{LOGLEVEL:level}\] %{GREEDYDATA:log_message}' ]
      tag_on_failure: [ "_no_line_level" ]
  - date:
      match: [ "ts", "ISO8601" ]
      target: "@timestamp"
  - lowercase_strings:
      keys: [ "level" ]
  - drop_events:
      when:
        equals:
        - ${level}
        - "debug"
  sink:
  - opensearch:
      hosts: [ "${OPENSEARCH_HOSTS:https://localhost:9200}" ]
      username: "${OPENSEARCH_USERNAME:admin}"
      password: "${OPENSEARCH_PASSWORD:admin}"
      index: "${OS_INDEX_FILTER_LEVEL:logs-level-detected}"
      ssl: ${OS_SSL:true}
      insecure: ${OS_INSECURE:false}
